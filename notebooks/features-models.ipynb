{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_dataset, home\n",
    "\n",
    "raw = home / 'data' / 'interim'\n",
    "raw = load_dataset(raw)\n",
    "\n",
    "comb = raw['combined']\n",
    "\n",
    "comb.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "split = TimeSeriesSplit(n_splits=2)\n",
    "\n",
    "subsample = -1\n",
    "comb = comb.iloc[:subsample, :]\n",
    "\n",
    "for tr_idx, te_idx in split.split(comb):\n",
    "    tr = comb.iloc[tr_idx, :]\n",
    "    te = comb.iloc[te_idx, :]\n",
    "\n",
    "assert tr.shape[1] == te.shape[1]\n",
    "assert tr.shape[0] + te.shape[0] == comb.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_sample(sample):\n",
    "    sample = sample.lower()\n",
    "    sample = sample.replace('/n', '')\n",
    "    sample = sample.replace(\"\\'\", '')\n",
    "    doc = nlp(sample)\n",
    "    lemmas = [token.lemma_ for token in doc if (not token.is_stop and not token.is_punct and not token.is_space)]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipe(combined):\n",
    "    corpus = combined.iloc[:, :-1]\n",
    "    corpus = corpus.agg(' '.join, axis=1)\n",
    "    corpus = corpus.apply(clean_sample)\n",
    "    corpus = corpus.to_frame()\n",
    "    corpus.columns = ['news']\n",
    "    target = combined.loc[:, 'final-label'].to_frame()\n",
    "    target.columns = ['target']\n",
    "    return corpus, target\n",
    "\n",
    "x_tr, y_tr = train_pipe(tr)\n",
    "x_te, y_te = train_pipe(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def fit(mdl, x_tr, y_tr, x_te, y_te, vec=None):\n",
    "    if vec:\n",
    "        x_tr = vec.fit_transform(x_tr.loc[:, 'news'])\n",
    "        x_te = vec.transform(x_te.loc[:, 'news'])\n",
    "        \n",
    "    y_tr = y_tr.values.flatten()\n",
    "    y_te = y_te.values.flatten()\n",
    "        \n",
    "    mdl.fit(x_tr, y_tr)\n",
    "    \n",
    "    res = {\n",
    "        'tr-score': mdl.score(x_tr, y_tr),\n",
    "        'te-score': mdl.score(x_te, y_te),\n",
    "        'avg-te-pred': np.mean(mdl.predict(x_te))\n",
    "    }\n",
    "    \n",
    "    for k, v in res.items():\n",
    "        print(k, v)\n",
    "    \n",
    "    return mdl, res\n",
    "\n",
    "rf = fit(\n",
    "    RandomForestClassifier(n_estimators=500, max_features=5),\n",
    "    x_tr, y_tr, x_te, y_te, vec=TfidfVectorizer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = fit(\n",
    "    LogisticRegression(C=5),\n",
    "    x_tr, y_tr, x_te, y_te, vec=TfidfVectorizer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "def clean_strings(docs):\n",
    "    tokens = []\n",
    "    for doc in docs:\n",
    "        doc = remove_stopwords(doc)\n",
    "        tokens.append(list(tokenize(doc, lower=True)))\n",
    "    return tokens\n",
    "        \n",
    "tr_tokens = clean_strings(x_tr.loc[:, 'news'].values)\n",
    "te_tokens = clean_strings(x_te.loc[:, 'news'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tr_tokens)]\n",
    "model = Doc2Vec(documents, vector_size=32, window=3, min_count=1, workers=4, verbose=1)\n",
    "\n",
    "def get_doc_vecs(docs, model):\n",
    "    vecs = []\n",
    "    for sample in docs:\n",
    "        vecs.append(model.infer_vector(sample))\n",
    "    return np.array(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vecs = get_doc_vecs(tr_tokens, model)\n",
    "te_vecs = get_doc_vecs(te_tokens, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = fit(\n",
    "    RandomForestClassifier(n_estimators=1000, max_features=5, max_depth=5),\n",
    "    tr_vecs, y_tr, te_vecs, y_te\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dates = pd.get_dummies(pd.to_datetime(x_tr.index).dayofweek)\n",
    "te_dates = pd.get_dummies(pd.to_datetime(x_te.index).dayofweek)\n",
    "\n",
    "rf = fit(\n",
    "    RandomForestClassifier(n_estimators=1000, max_features=5),\n",
    "    tr_dates, y_tr, te_dates, y_te\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Senitment & subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def sentiment(row):\n",
    "    return TextBlob(row).sentiment.polarity\n",
    "\n",
    "def subjectivity(row):\n",
    "    return TextBlob(row).sentiment.subjectivity\n",
    "\n",
    "def add_sum(df):\n",
    "    df.loc[:, 'sent'] = df.loc[:, 'news'].apply(sentiment)\n",
    "    df.loc[:, 'subj'] = df.loc[:, 'news'].apply(subjectivity)\n",
    "    return df\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment(row):\n",
    "    return TextBlob(row).sentiment.polarity\n",
    "\n",
    "def subjectivity(row):\n",
    "    return TextBlob(row).sentiment.subjectivity\n",
    "\n",
    "x_s_tr = add_sum(x_tr).drop('news', axis=1)\n",
    "x_s_te = add_sum(x_te).drop('news', axis=1)\n",
    "\n",
    "rf = fit(\n",
    "    RandomForestClassifier(n_estimators=500, max_features=None, max_depth=3),\n",
    "    x_s_tr, y_tr, x_s_te, y_te\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entity extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entities(sample):\n",
    "    doc = nlp(sample)\n",
    "\n",
    "    ents = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PROPN' and token.tag_ == 'NNP':\n",
    "            ents.append(token.text)\n",
    "\n",
    "    return np.array(ents).reshape(1, -1)\n",
    "\n",
    "def generate_ents(df):\n",
    "    tokens = []\n",
    "    for row in range(df.shape[0]):\n",
    "        sample = df.iloc[row, :].loc['news']\n",
    "        tokens.append(find_entities(sample))\n",
    "        \n",
    "    assert len(tokens) == df.shape[0]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def extract_ents_as_str(ents):\n",
    "    only_str = []\n",
    "    for row in ents:\n",
    "        only_str.append(\" \".join(row.flatten().tolist()))\n",
    "    return only_str\n",
    "\n",
    "tr_ents = generate_ents(x_tr)\n",
    "te_ents = generate_ents(x_te)\n",
    "\n",
    "tr_ents = extract_ents_as_str(tr_ents)\n",
    "te_ents = extract_ents_as_str(te_ents)\n",
    "\n",
    "enc = CountVectorizer()\n",
    "tr_ents = enc.fit_transform(tr_ents)\n",
    "te_ents = enc.transform(te_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = fit(\n",
    "    RandomForestClassifier(n_estimators=500, max_features=None, max_depth=4),\n",
    "    tr_ents, y_tr, te_ents, y_te\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
